{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connectivity-based searchlight hyperalignment_tutorial     \n",
    "### Erica Busch, 7/20\n",
    "This is an example of how to run searchlight hyperalignment on a dataset. The example dataset here is the Grand   Budapest Hotel dataset, which has 5 runs for each of 21 subjects. This data has already been preprocessed and aligned to the fsaverage surface.   \n",
    "\n",
    "In this example, we will compute the sparse connectivity matrices based on the first 4 runs of data. We then apply searchlight hyperalignment with a 20mm searchlight radius to these matrices to derive the common model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,glob\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from mvpa2.datasets.base import Dataset\n",
    "from mvpa2.misc.surfing.queryengine import SurfaceQueryEngine\n",
    "from mvpa2.mappers.fxy import FxyMapper\n",
    "from mvpa2.algorithms.searchlight_hyperalignment import SearchlightHyperalignment\n",
    "from mvpa2.base import debug\n",
    "from mvpa2.base.hdf5 import h5save, h5load\n",
    "import dataset_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/dartfs/rc/lab/D/DBIC/DBIC/f002d44/budapest/transformations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in PyMVPA datasets for each participant's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define what nodes are included in each searchlight using Dijkstra's distance metric\n",
    "In general, I create these files once and save them to be reloaded each time I need them, which is often.  \n",
    "This function creates a list of lists where the 0th element of each nested list is the index of the node around which the searchlight is built. The remaining elements in that list are nodes that fall within a searchlight on the given surface.\n",
    "\n",
    "For more information, look up the documentation on PyMVPA SurfaceQueryEngine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input one subject's dataset (as a PyMVPA dataset), \n",
    "# which we're just using as a template, the target hemisphere, the searchlight radius, \n",
    "# and where you want to save this file. \n",
    "def compute_searchlights(ds, hemi, radius, outdir):\n",
    "    from mvpa2.misc.surfing.queryengine import SurfaceQueryEngine\n",
    "    # get the data for jsut the first participant\n",
    "    node_indices = utils.get_node_indices(hemi)\n",
    "    surf = get_freesurfer_surfaces(hemi)\n",
    "    ds.fa['node_indices'] = node_indices.copy()\n",
    "    qe = SurfaceQueryEngine(surf, radius)\n",
    "    qe.train(ds)\n",
    "\n",
    "    searchlights = []\n",
    "    for idx in node_indices:\n",
    "        sl = qe.query_byid(idx)\n",
    "        searchlights.append(sl)\n",
    "    savename = os.path.join(outdir,'{R}mm_searchlights_{S}h.npy'.format(R=radius,S=hemi))\n",
    "    np.save(savename, searchlights)\n",
    "    print('saved at: '+str(savename))\n",
    "    return searchlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function just loads and returns your searchlights.\n",
    "# since we did this on a hemisphere by hemisphere basis, we have to adjust if we're going to \n",
    "# use \n",
    "def load_searchlights(radius,hemi):\n",
    "    if hemi == 'b':\n",
    "        lh = load_searchlights(radius,'l')\n",
    "        rh = load_searchlights(radius, 'r')\n",
    "        adjusted_sls_rh = []\n",
    "        for sl in rh:\n",
    "            adjusted_sls_rh.append([b + len(lh) for b in sl])\n",
    "        return np.concatenate((lh,adjusted_sls_rh),axis=0)\n",
    "    return np.load(utils.basedir+'/{R}mm_searchlights_{S}h.npy'.format(R=radius,S=hemi),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define our target and seed node indices\n",
    "To create connectivity targets, we will use 20mm searchlights defined on a sparse cortical surface (ico3, 642 nodes/hemisphere). Our connectivity seeds correspond to every vertex on the cortical surface of our data (in this case, ico5, 10242 nodes/hemisphere)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the node indices that will be used for creating connectivity targets\n",
    "# defined on the sparse cortical surface after masking out the medial wall. This returns two lists:\n",
    "# target_node_idx[0] is a list of the 588 node indices that are included in the left hemisphere\n",
    "# and target_node_idx[1] are the 587 node indices in right hemisphere. \n",
    "sparse_resolution = 642\n",
    "target_node_idx = utils.get_node_indices(hemi='b', surface_res=sparse_resolution)\n",
    "\n",
    "\n",
    "# now we define all of the 'seed' node indices, which are all the nodes on the surface of our data\n",
    "# after masking the medial wall. \n",
    "# this gives us two lists: seed_node_idx[0] are the 9372 nodes lh \n",
    "# seed_node_idx[1] are the 9370 rh\n",
    "dense_resolution = 10242\n",
    "seed_node_idx = utils.get_node_indices(hemi='b', surface_res=dense_resolution) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define our connectivity target function\n",
    "We will average each time point across all nodes in the searchlight to get an average time series response for the searchlight. \n",
    "This 'mean feature measure' for each searchlight will serve as the connectivity targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass this function a single subject's dataset (for that hemisphere, will be defined in thee function that calls this function) \n",
    "# and the list of searchlights for that hemisphere. \n",
    "# returns a numpy array where each item represents the average timecourse\n",
    "# of all the nodes in that searchlight.\n",
    "def compute_mean_features(ds, searchlights):\n",
    "\tmean_features = []\n",
    "\tfor sl in searchlights:\n",
    "\t\tm = np.mean(ds[:,sl],axis=1)\n",
    "\t\tmean_features.append(zscore(m))\n",
    "\treturn np.array(mean_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the function that builds our connectivity matrices.\n",
    "This function takes as input all of your datasets (loaded at the top, `dss`), your targets and seeds, and an out-directory (`outdir`) which can be `none` or a path where you want to save this to memory or just return the connectome.   \n",
    "A lot of this code is based on this: https://github.com/PyMVPA/PyMVPA/blob/master/mvpa2/algorithms/connectivity_hyperalignment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_connectivity_matrix(dss, seed_idx, target_idx, outdir=None):\n",
    "    targets_lh, targets_rh = target_idx[0], target_idx[1]\n",
    "    searchlights_lh = load_searchlights(radius, 'l') #typically we use a 13 mm searchlight\n",
    "    searchlights_rh = load_searchlights(radius, 'r')\n",
    "    # define our connectivity metric, the dot product of samples (which on zscored data becomes\n",
    "    # correlation if you normalize by nsamples.\n",
    "    conn_metric = lambda x,y: np.dot(x.samples, y.samples)/x.nsamples\n",
    "    connectivity_mapper = FxyMapper(conn_metric)\n",
    "    # loop through each participant & save their connectivity matrix as a pymvpa dataset so we can just stick it into the \n",
    "    # searchlight hyperalignment algorithm. \n",
    "    connectomes = []\n",
    "    for ds,subj in zip(dss,utils.subjects):\n",
    "        if isinstance(ds,np.ndarray):\n",
    "            ds = Dataset(samples=ds)\n",
    "        ds.fa['node_indices'] = np.concatenate(seed_idx) # bc seed_idx is a list of two arrays (1/hemi)\n",
    "        # gets mean feature measure for each searchlight on the sparse surface\n",
    "        mean_features_lh = compute_mean_features(ds.samples[:,:9372],searchlights_lh[:len(targets_lh)]) \n",
    "        mean_features_rh = compute_mean_features(ds.samples[:,9372:],searchlights_rh[:len(targets_rh)])\n",
    "        mean_features = np.vstack((mean_features_lh,mean_features_rh))\n",
    "        print('mean features of shape: '+str(mean_features.shape)) # this will be (1175,n_timepoints)\n",
    "        conn_targets = Dataset(samples=mean_features)\n",
    "        conn_targets.sa['target_ids'] = np.concatenate((targets_lh,targets_rh),axis=0)\n",
    "        print('getting conn vectors for subj {s}'.format(s=subj))\n",
    "        connectivity_mapper.train(conn_targets)\n",
    "        connectome = connectivity_mapper.forward(ds)\n",
    "        print(connectome.shape)\n",
    "        connectomes.append(connectome)\n",
    "    print('connectomes of shape: '+str(len(connectomes))+str(connectomes[0].shape))\n",
    "    if outdir:\n",
    "        outstr = outdir+'/connectomes_{b}targets.npy'.format(b=len(target_ids))\n",
    "        np.save(outstr, connectomes)\n",
    "        print('saved at: '+outstr)\n",
    "    return connectomes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectomes = build_connectivity_matrix(dss, seed_idx, target_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get everything in shape for hyperalignment training\n",
    "Make sure our connectomes are a list of pymvpa datasets (1/subject) with labeled node indices\n",
    "and are zscored  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in connectomes:\n",
    "    if isinstance(c,np.ndarray):\n",
    "        c = Dataset(samples=c)\n",
    "    c.fa['node_indices'] = np.concatenate(seed_idx).copy()\n",
    "    c.samples = zscore(c.samples,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a query engine and load your surfaces\n",
    "surface = utils.get_freesurfer_surface()\n",
    "radius = 20 \n",
    "qe = SurfaceQueryEngine(surface, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Now we're ready to run searchlight hyperalignment\n",
    "Let's time it and also activate the debugger so we can track its progress.  \n",
    "Then, we create an instance of searchlight hyperalignment and apply it to get our   \n",
    "transformation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a9c2c982b99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# where to write out intermediate files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# it's arbitrary that I set all these variables; i am just lazy and do not remember what the environ variable is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TMPDIR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/dartfs-hpc/scratch/f002d44/temp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TEMP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/dartfs-hpc/scratch/f002d44/temp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TMP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/dartfs-hpc/scratch/f002d44/temp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# First, let's make sure that we're pointing our intermediate, temporary file writing to our scratch directory.\n",
    "# where to write out intermediate files\n",
    "# it's arbitrary that I set all these variables; i am just lazy and do not remember what the environ variable is\n",
    "os.environ['TMPDIR'] = '/dartfs-hpc/scratch/f002d44/temp'\n",
    "os.environ['TEMP'] = '/dartfs-hpc/scratch/f002d44/temp'\n",
    "os.environ['TMP'] = '/dartfs-hpc/scratch/f002d44/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print('-------- beginning hyperalignment at {t0} --------'.format(t0=t0))\n",
    "debug.active += ['SHPAL', 'SLC']\n",
    "\n",
    "N_PROCS=16\n",
    "N_BLOCKS=128\n",
    "\n",
    "slhyper = SearchlightHyperalignment(queryengine=qe, # pass it our surface query engine\n",
    "\t\t\t\t\t\t\t\t\tnproc=N_PROCS, # the number of processes we want to use\n",
    "\t\t\t\t\t\t\t\t\tnblocks=N_BLOCKS, # the number of blocks we want to divide that into (the more you have the less memory it takes)\n",
    "\t\t\t\t\t\t\t\t\tmask_node_ids=node_indices, # tell it which nodes you are masking \n",
    "\t\t\t\t\t\t\t\t\tdtype ='float64')\n",
    "\n",
    "transformations = slhyper(cnx)\n",
    "elapsed = time.time()-t0\n",
    "print('-------- time elapsed: {elapsed} --------'.format(elapsed=elapsed))\n",
    "h5save(outdir+'connectivity_hyperalignment_mappers.hdf5.gz', transformations, compression=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. You did it! Way to go. \n",
    "That saved a HDF5 file of each subject's transformation matrices into the common space. \n",
    "Now we save each individual's mapper as a npz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "transformations = h5load(outdir+'connectivity_hyperalignment_mappers.hdf5.gz')\n",
    "for T, subj in zip(transformations, utils.subjects):\n",
    "\tsave_npz(outdir+'sub{s}_ha_mapper.npz'.format(s=subj), T._proj)\n",
    "print('done saving individual mappers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Now we are going to apply these individual mappers to our test data to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_data(train=False) # get our test runs\n",
    "aligned_data  = []\n",
    "for subj, ds in zip(budapest_subjects, test_data):\n",
    "\tT = load_npz(outdir+'sub{s}_ha_mapper.npz'.format(s=subj))\n",
    "\tprint(ds.shape, ds.dtype, T.shape, T.dtype)\n",
    "\taligned = np.nan_to_num(zscore((np.asmatrix(ds) * T).A, axis=0)) # apply the transformation\n",
    "\tnp.save(outdir+'sub{s}_cnx_hyperaligned_data.npy'.format(s=subj), aligned) # or you can save left and right hemispheres separately if you so desire.\n",
    "\tprint('done with subj {s}'.format(s=subj))\n",
    "    aligned_data.append(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertex_isc(data):\n",
    "    all_results = []\n",
    "    all_subjs = np.arange(data.shape[0])\n",
    "    # loop through all vertices\n",
    "    for v in np.arange(data.shape[2]):\n",
    "        results = []\n",
    "        data_v = data[:,:,v]\n",
    "        for subj, ds in enumerate(data_v):\n",
    "            group = np.setdiff1d(all_subjs, subj) # make groups\n",
    "            group_avg = np.mean(data_v[group,:], axis=0).ravel()\n",
    "            r = np.corrcoef(group_avg, ds.ravel())[0,1]\n",
    "            results.append(r)\n",
    "        all_results.append(np.mean(np.array(results)))\n",
    "    res = np.array(all_results)\n",
    "    np.save(outdir+'/vertex_isc.npy', res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_results = vertex_isc(aligned_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
