{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperalignment_tutorial     \n",
    "### Erica Busch, 5/2020  \n",
    "This is an example of how to run searchlight hyperalignment on a dataset. The example dataset here is the Grand   Budapest Hotel dataset, which has 5 runs for each of 21 subjects. This data has already been preprocessed and aligned to the fsaverage surface.   \n",
    "\n",
    "In this example we will apply searchlight hyperalignment with a 20mm searchlight radius to the response profiles. We train the hyperalignment algorithm on the first 4 runs and then test the transformations on the 5th run.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,glob\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from mvpa2.datasets.base import Dataset\n",
    "from mvpa2.misc.surfing.queryengine import SurfaceQueryEngine\n",
    "from mvpa2.algorithms.searchlight_hyperalignment import SearchlightHyperalignment\n",
    "from mvpa2.base import debug\n",
    "from mvpa2.base.hdf5 import h5save, h5load\n",
    "import dataset_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/dartfs/rc/lab/D/DBIC/DBIC/f002d44/budapest/transformations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create PyMVPA datasets for each participant's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = utils.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load your surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface = utils.get_freesurfer_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the indices of your data that you want to be included in hyperalignment. This relates your data back to the surface that you have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indices = utils.get_node_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inject those node indices into your dataset as feature attributes.\n",
    "This basically is labeling your features.  \n",
    "If you're paranoid, you can zscore your data again here.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dss:\n",
    "    d.fa['node_indices']=node_indices.copy()\n",
    "    # I print this out here just to double check again. \n",
    "    # If it looks like my data hasn't been zscored, I will again.\n",
    "    print(d.shape, np.min(d), np.mean(d), np.max(d)) \n",
    "    # d.samples = zscore(d.samples,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next, we build a surface query engine, a thingymabooper that creates searchlights on your surface according to your radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 20 \n",
    "qe = SurfaceQueryEngine(surface, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. We're ready for hyperalignment! Let's time it and also activate the debugger so we can track its progress. Then, we create an instance of searchlight hyperalignment and apply it to get our transformation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make sure that we're pointing our intermediate, temporary file writing to our scratch directory.\n",
    "# where to write out intermediate files\n",
    "os.environ['TMPDIR'] = '/dartfs-hpc/scratch/f002d44/temp'\n",
    "os.environ['TEMP'] = '/dartfs-hpc/scratch/f002d44/temp'\n",
    "os.environ['TMP'] = '/dartfs-hpc/scratch/f002d44/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print('-------- beginning hyperalignment at {t0} --------'.format(t0=t0))\n",
    "debug.active += ['SHPAL', 'SLC']\n",
    "\n",
    "N_PROCS=16\n",
    "N_BLOCKS=128\n",
    "\n",
    "slhyper = SearchlightHyperalignment(queryengine=qe, # pass it our surface query engine\n",
    "\t\t\t\t\t\t\t\t\tnproc=N_PROCS, # the number of processes we want to use\n",
    "\t\t\t\t\t\t\t\t\tnblocks=N_BLOCKS, # the number of blocks we want to divide that into (the more you have the less memory it takes)\n",
    "\t\t\t\t\t\t\t\t\tmask_node_ids=node_indices, # tell it which nodes you are masking \n",
    "\t\t\t\t\t\t\t\t\tdtype ='float64')\n",
    "\n",
    "transformations = slhyper(dss)\n",
    "elapsed = time.time()-t0\n",
    "print('-------- time elapsed: {elapsed} --------'.format(elapsed=elapsed))\n",
    "h5save(outdir+'hyperalignment_mappers.hdf5.gz', transformations, compression=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. You did it! Way to go. That saved a HDF5 file of each subject's transformation matrices into the common space. Now we save each individual's mapper as a npz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "transformations = h5load(outdir+'hyperalignment_mappers.hdf5.gz')\n",
    "for T, subj in zip(transformations, budapets_subjects):\n",
    "\tsave_npz(outdir+'sub{s}_ha_mapper.npz'.format(s=subj), T._proj)\n",
    "print('done saving individual mappers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Now we are going to apply these individual mappers to our test data to validate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_data(train=False) # get our test runs\n",
    "for subj, ds in zip(budapest_subjects, test_data):\n",
    "\tT = load_npz(outdir+'sub{s}_ha_mapper.npz'.format(s=subj))\n",
    "\tprint(ds.shape, ds.dtype, T.shape, T.dtype)\n",
    "\taligned = np.nan_to_num(zscore((np.asmatrix(ds) * T).A, axis=0)) # apply the transformation\n",
    "\tnp.save(outdir+'sub{s}_hyperaligned_data.npy'.format(s=subj), aligned) # or you can save left and right hemispheres separately if you so desire.\n",
    "\tprint('done with subj {s}'.format(s=subj))\n",
    "print('DONEZO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Now you can implement whatever test of fit you want -- between subject classification, intersubject correlation, whatever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertex_isc(data):\n",
    "    all_results = []\n",
    "    all_subjs = np.arange(data.shape[0])\n",
    "    # loop through all vertices\n",
    "    for v in np.arange(data.shape[2]):\n",
    "        results = []\n",
    "        data_v = data[:,:,v]\n",
    "        for subj, ds in enumerate(data_v):\n",
    "            group = np.setdiff1d(all_subjs, subj) # make groups\n",
    "            group_avg = np.mean(data_v[group,:], axis=0).ravel()\n",
    "            r = np.corrcoef(group_avg, ds.ravel())[0,1]\n",
    "            results.append(r)\n",
    "        all_results.append(np.mean(np.array(results)))\n",
    "    res = np.array(all_results)\n",
    "    print(res.shape)\n",
    "    np.save(outdir+'/vertex_isc.npy', res)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 anaconda3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
